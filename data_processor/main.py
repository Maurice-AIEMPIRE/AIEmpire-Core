"""
Empire Data Processor â€” Hauptprozess
=====================================
Orchestriert den kompletten Pipeline-Flow:
  Watch â†’ Extract â†’ Analyze (3-Layer AI) â†’ Format â†’ Output â†’ iCloud Push

Starten:
  python3 data_processor/main.py               # Daemon-Modus (Watch + Auto-Prozess)
  python3 data_processor/main.py process <file> # Einzelne Datei verarbeiten
  python3 data_processor/main.py batch          # Alle Dateien in /data/input/ verarbeiten
  python3 data_processor/main.py status         # Pipeline-Status anzeigen
  python3 data_processor/main.py push           # Ergebnisse nach iCloud pushen
"""

import asyncio
import logging
import os
import shutil
import sys
from pathlib import Path

# â”€â”€â”€ Projekt-Root im Pfad â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_HERE = Path(__file__).parent
_ROOT = _HERE.parent
sys.path.insert(0, str(_ROOT))

from data_processor.watch_daemon import WatchDaemon
from data_processor.processors import (
    PDFProcessor,
    ImageProcessor,
    AudioProcessor,
    JSONProcessor,
    CSVProcessor,
    OfficeProcessor,
)
from data_processor.analyzer import EmpireAnalyzer
from data_processor.output_formatter import OutputFormatter
from data_processor.sftp_bridge import (
    pipeline_status,
    push_results_to_mac,
    generate_index,
    SERVER_INPUT_DIR,
    SERVER_OUTPUT_DIR,
    SERVER_PROCESSED_DIR,
)
from data_processor.database import (
    init_db,
    save_document,
    already_processed,
    run_exports,
    get_stats,
    DB_PATH,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s â€” %(message)s",
)
logger = logging.getLogger("empire.pipeline")

# â”€â”€â”€ Dateityp â†’ Prozessor-Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROCESSOR_MAP: dict[str, type] = {
    ".pdf":  PDFProcessor,
    ".jpg":  ImageProcessor,
    ".jpeg": ImageProcessor,
    ".png":  ImageProcessor,
    ".gif":  ImageProcessor,
    ".bmp":  ImageProcessor,
    ".mp3":  AudioProcessor,
    ".wav":  AudioProcessor,
    ".m4a":  AudioProcessor,
    ".ogg":  AudioProcessor,
    ".flac": AudioProcessor,
    ".json": JSONProcessor,
    ".csv":  CSVProcessor,
    ".tsv":  CSVProcessor,
    ".docx": OfficeProcessor,
    ".xlsx": OfficeProcessor,
    ".pptx": OfficeProcessor,
    ".doc":  OfficeProcessor,
    ".xls":  OfficeProcessor,
    ".txt":  JSONProcessor,   # txt als einfacher Text
}


def get_processor(file_path: Path):
    """WÃ¤hle richtigen Prozessor basierend auf Dateiendung."""
    suffix = file_path.suffix.lower()
    cls = PROCESSOR_MAP.get(suffix, JSONProcessor)
    return cls(str(file_path))


# â”€â”€â”€ Haupt-Prozess-Funktion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EmpireDataProcessor:
    """Pipeline-Orchestrator: Watch â†’ Extract â†’ Analyze â†’ Output â†’ iCloud"""

    def __init__(
        self,
        input_dir: str | None = None,
        output_dir: str | None = None,
        processed_dir: str | None = None,
        auto_push: bool = False,
    ):
        self.input_dir = Path(input_dir or SERVER_INPUT_DIR)
        self.output_dir = Path(output_dir or SERVER_OUTPUT_DIR)
        self.processed_dir = Path(processed_dir or SERVER_PROCESSED_DIR)

        # Verzeichnisse anlegen
        for d in [self.input_dir, self.output_dir, self.processed_dir]:
            d.mkdir(parents=True, exist_ok=True)

        self.analyzer = EmpireAnalyzer()
        self.formatter = OutputFormatter(str(self.output_dir))
        self.daemon = WatchDaemon(str(self.input_dir), self.process_file)
        self.auto_push = auto_push
        self._processed_count = 0

        # Datenbank initialisieren
        init_db(DB_PATH)

    async def start(self):
        """Starte als Daemon (Watch-Modus)."""
        logger.info("=" * 60)
        logger.info("  Empire Data Pipeline â€” Daemon gestartet")
        logger.info(f"  Input:     {self.input_dir}")
        logger.info(f"  Output:    {self.output_dir}")
        logger.info(f"  Processed: {self.processed_dir}")
        logger.info(f"  Auto-Push: {self.auto_push}")
        logger.info("=" * 60)
        await self.daemon.start()

    async def stop(self):
        """Stoppe Daemon."""
        await self.daemon.stop()
        logger.info(f"Pipeline gestoppt â€” {self._processed_count} Dateien verarbeitet")

    async def process_file(self, file_path: str) -> dict | None:
        """
        Kompletter Flow fÃ¼r eine Datei:
          1. Extract  â€” Rohinhalt lesen
          2. Analyze  â€” 3-Layer AI (Qwen â†’ DeepSeek â†’ Verify)
          3. Format   â€” Markdown + JSON in iCloud-Ordner
          4. Archive  â€” Originaldatei â†’ /data/processed/
          5. Push     â€” Optional: Ergebnisse â†’ Mac/iCloud
        """
        fp = Path(file_path)
        if not fp.exists():
            logger.error(f"Datei nicht gefunden: {fp}")
            return None

        logger.info(f"\n{'â”€'*50}")
        logger.info(f"ðŸ“¥ Verarbeite: {fp.name}")

        # Duplikat-Check (Hash-basiert)
        if already_processed(str(fp)):
            logger.info(f"  â­  Bereits in DB â€” Ã¼berspringe: {fp.name}")
            archive_path = self.processed_dir / fp.name
            shutil.move(str(fp), str(archive_path))
            return {"file": fp.name, "status": "duplicate_skipped"}

        try:
            # Phase 1: Extract
            processor = get_processor(fp)
            extracted = await processor.process()
            logger.info(f"  âœ“ Extrahiert: {extracted.get('content_type', '?')}")

            # Phase 2: Analyze (3 Layers: Qwen â†’ DeepSeek â†’ Cross-Verify)
            analysis = await self.analyzer.analyze(extracted)
            icloud_folder = analysis.get("final", {}).get("icloud_folder", "Sonstiges")
            importance = analysis.get("final", {}).get("importance", "?")
            logger.info(f"  âœ“ Analysiert: [{importance}] â†’ {icloud_folder}/")

            # Phase 3: In Datenbank speichern
            doc_id = save_document(str(fp), extracted, analysis)
            logger.info(f"  âœ“ Datenbank: ID #{doc_id}")

            # Phase 4: Format & Save (Markdown + JSON in iCloud-Ordner)
            outputs = await self.formatter.format_and_save(extracted, analysis)
            logger.info(f"  âœ“ Gespeichert: {outputs['markdown']}")

            # Phase 5: Archive Original
            archive_path = self.processed_dir / fp.name
            shutil.move(str(fp), str(archive_path))
            logger.info(f"  âœ“ Archiviert: {archive_path}")

            self._processed_count += 1

            # Phase 6: Auto-Push nach iCloud (optional)
            if self.auto_push:
                run_exports()           # DB-Exports (JSON, CSV, Markdown)
                generate_index(self.output_dir)
                await push_results_to_mac(self.output_dir)

            logger.info(f"  âœ… FERTIG: {fp.name} ({analysis.get('total_analysis_time_s', '?')}s)")
            return {
                "file": fp.name,
                "db_id": doc_id,
                "icloud_folder": icloud_folder,
                "importance": importance,
                "outputs": outputs,
            }

        except Exception as e:
            logger.error(f"  âŒ FEHLER bei {fp.name}: {e}", exc_info=True)
            return {"file": fp.name, "error": str(e)}

    async def process_batch(self) -> list[dict]:
        """Verarbeite alle Dateien die gerade in input_dir liegen."""
        files = [
            f for f in self.input_dir.iterdir()
            if f.is_file() and not f.name.startswith(".")
        ]
        if not files:
            logger.info("Keine Dateien in input_dir gefunden")
            return []

        logger.info(f"Batch: {len(files)} Dateien")
        results = []
        for f in files:
            result = await self.process_file(str(f))
            if result:
                results.append(result)

        # DB-Exports + Index + Push nach Batch
        run_exports()
        generate_index(self.output_dir)
        stats = get_stats()
        logger.info(f"\nBatch fertig: {len(results)}/{len(files)} erfolgreich")
        logger.info(f"Datenbank: {stats['total_documents']} Dokumente gesamt")
        if self.auto_push:
            await push_results_to_mac(self.output_dir)

        return results


# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main():
    cmd = sys.argv[1] if len(sys.argv) > 1 else "daemon"

    if cmd == "status":
        import json
        init_db()
        status = pipeline_status()
        status["database"] = get_stats()
        print(json.dumps(status, indent=2, ensure_ascii=False))

    elif cmd == "db":
        import json
        init_db()
        sub = sys.argv[2] if len(sys.argv) > 2 else "stats"
        if sub == "stats":
            print(json.dumps(get_stats(), indent=2, ensure_ascii=False))
        elif sub == "export":
            paths = run_exports()
            for fmt, path in paths.items():
                print(f"{fmt}: {path}")
        elif sub == "search":
            from data_processor.database import search
            q = " ".join(sys.argv[3:]) if len(sys.argv) > 3 else "*"
            results = search(q)
            for r in results:
                print(f"[{r['importance']}] {r['file_name']} â†’ {r['icloud_folder']} â€” {(r['summary'] or '')[:80]}")

    elif cmd == "push":
        generate_index()
        result = await push_results_to_mac()
        import json
        print(json.dumps(result, indent=2, ensure_ascii=False))

    elif cmd == "index":
        path = generate_index()
        print(f"Index erstellt: {path}")

    elif cmd == "process" and len(sys.argv) > 2:
        file_arg = sys.argv[2]
        processor = EmpireDataProcessor()
        result = await processor.process_file(file_arg)
        import json
        print(json.dumps(result, indent=2, ensure_ascii=False, default=str))

    elif cmd == "batch":
        auto_push = "--push" in sys.argv
        processor = EmpireDataProcessor(auto_push=auto_push)
        results = await processor.process_batch()
        import json
        print(json.dumps(results, indent=2, ensure_ascii=False, default=str))

    elif cmd in ("daemon", "watch", "start"):
        auto_push = "--push" in sys.argv
        processor = EmpireDataProcessor(auto_push=auto_push)
        try:
            await processor.start()
            while True:
                await asyncio.sleep(5)
        except KeyboardInterrupt:
            logger.info("\nStoppe...")
            await processor.stop()

    else:
        print(__doc__)


if __name__ == "__main__":
    asyncio.run(main())
