# ═══════════════════════════════════════════════════════════════
# LiteLLM Proxy Configuration
# Unified API gateway for all AI providers
# ═══════════════════════════════════════════════════════════════

model_list:
  # ─── Google Gemini (Primary Cloud Provider) ─────────────────
  - model_name: gemini-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-pro
    litellm_params:
      model: gemini/gemini-2.0-pro
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-thinking
    litellm_params:
      model: gemini/gemini-2.0-flash-thinking
      api_key: os.environ/GEMINI_API_KEY

  # ─── OpenAI ────────────────────────────────────────────────
  - model_name: gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY

  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY

  # ─── Groq (Fast Cloud Inference) ───────────────────────────
  - model_name: groq-llama
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY

  - model_name: groq-mixtral
    litellm_params:
      model: groq/mixtral-8x7b-32768
      api_key: os.environ/GROQ_API_KEY

  # ─── Anthropic/Claude ──────────────────────────────────────
  - model_name: claude-haiku
    litellm_params:
      model: claude-haiku-4-5-20251001
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-sonnet
    litellm_params:
      model: claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY

  # ─── Ollama (Local - Free, Offline) ─────────────────────────
  - model_name: qwen-14b
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://ollama:11434

  - model_name: qwen-7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://ollama:11434

  - model_name: mistral-7b
    litellm_params:
      model: ollama/mistral:7b
      api_base: http://ollama:11434

  - model_name: deepseek-r1
    litellm_params:
      model: ollama/deepseek-r1:7b
      api_base: http://ollama:11434

  # ─── Moonshot/Kimi (Backup Cloud) ───────────────────────────
  - model_name: kimi
    litellm_params:
      model: openai/kimi-k2.5
      api_key: os.environ/MOONSHOT_API_KEY
      api_base: https://api.moonshot.ai/v1

# ─── Router Settings ─────────────────────────────────────────
router_settings:
  routing_strategy: "least-busy"
  num_retries: 2
  timeout: 120
  allowed_fails: 3
  cooldown_time: 60

# ─── Fallback Configuration ──────────────────────────────────
litellm_settings:
  fallbacks:
    - gemini-flash:
        - groq-llama
        - qwen-7b
        - kimi
    - gemini-pro:
        - groq-llama
        - qwen-14b
        - kimi
    - gemini-thinking:
        - deepseek-r1
        - qwen-14b
    - gpt-4o-mini:
        - groq-llama
        - qwen-7b
    - gpt-4o:
        - gemini-pro
        - groq-llama
    - groq-llama:
        - qwen-14b
        - kimi
    - claude-haiku:
        - groq-llama
        - kimi
    - claude-sonnet:
        - gemini-pro
        - groq-llama
  set_verbose: false
  drop_params: true
  request_timeout: 120
