# ═══════════════════════════════════════════════════════════════
# LiteLLM Proxy Configuration
# Unified API gateway for all AI providers
# ═══════════════════════════════════════════════════════════════

model_list:
  # ─── Google Gemini (Primary Cloud Provider) ─────────────────
  - model_name: gemini-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-pro
    litellm_params:
      model: gemini/gemini-2.0-pro
      api_key: os.environ/GEMINI_API_KEY

  - model_name: gemini-thinking
    litellm_params:
      model: gemini/gemini-2.0-flash-thinking
      api_key: os.environ/GEMINI_API_KEY

  # ─── Ollama (Local - Free, Offline) ─────────────────────────
  - model_name: qwen-14b
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://ollama:11434

  - model_name: qwen-7b
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://ollama:11434

  - model_name: deepseek-r1
    litellm_params:
      model: ollama/deepseek-r1:7b
      api_base: http://ollama:11434

  # ─── Moonshot/Kimi (Backup Cloud) ───────────────────────────
  - model_name: kimi
    litellm_params:
      model: openai/kimi-k2.5
      api_key: os.environ/MOONSHOT_API_KEY
      api_base: https://api.moonshot.ai/v1

# ─── Router Settings ─────────────────────────────────────────
router_settings:
  routing_strategy: "least-busy"
  num_retries: 2
  timeout: 120
  allowed_fails: 3
  cooldown_time: 60

# ─── Fallback Configuration ──────────────────────────────────
litellm_settings:
  fallbacks:
    - gemini-flash:
        - qwen-7b
        - kimi
    - gemini-pro:
        - qwen-14b
        - kimi
    - gemini-thinking:
        - deepseek-r1
        - qwen-14b
  set_verbose: false
  drop_params: true
  request_timeout: 120
