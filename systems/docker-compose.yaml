# AIEMPIRE DOCKER STACK (Hetzner Server)
# Only services that run INSIDE Docker.
# Ollama, Redis, PostgreSQL run NATIVE (systemd) for better performance.

services:
  # ═══════════════════════════════════════════
  # 1. CHROMA - Vector Database (Embeddings)
  # ═══════════════════════════════════════════
  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G

  # ═══════════════════════════════════════════
  # 2. n8n - Workflow Automation
  # ═══════════════════════════════════════════
  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    ports:
      - "5678:5678"
    volumes:
      - n8n_data:/home/node/.n8n
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD:-changeme}
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://65.21.203.174:5678/
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G

  # ═══════════════════════════════════════════
  # 3. LiteLLM Proxy - Unified AI Gateway
  # ═══════════════════════════════════════════
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    ports:
      - "4000:4000"
    volumes:
      - /opt/aiempire/openclaw-config/litellm_config.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    env_file:
      - /opt/aiempire/.env
    depends_on:
      - chromadb
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G

volumes:
  chroma_data:
  n8n_data:

networks:
  default:
    name: aiempire-network
