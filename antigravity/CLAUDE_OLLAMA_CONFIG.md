# Claude Code â†’ Ollama Configuration

## ğŸ¯ Ziel

Claude Code soll mit lokalen Ollama-Modellen statt Cloud-API arbeiten.

## âš ï¸ Wichtiger Hinweis

Claude Code kann **nicht direkt** mit Ollama sprechen, da die APIs unterschiedlich sind.

**Aber:** Du kannst einen Proxy nutzen, der die Anthropic API auf Ollama umbiegt.

## ğŸ”§ Option 1: LiteLLM Proxy (Empfohlen)

LiteLLM ist ein Proxy, der verschiedene LLM-APIs (inkl. Ollama) unter einer einheitlichen API (OpenAI/Anthropic-kompatibel) verfÃ¼gbar macht.

### Installation

```bash
pip install litellm[proxy]
```

### Konfiguration

```bash
# Erstelle config.yaml
cat > litellm_config.yaml << 'EOF'
model_list:
  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
      model: ollama/qwen2.5-coder:14b
      api_base: http://localhost:11434
      
  - model_name: claude-3-5-haiku-20241022
    litellm_params:
      model: ollama/qwen2.5-coder:7b
      api_base: http://localhost:11434

  - model_name: claude-3-opus-20240229
    litellm_params:
      model: ollama/deepseek-r1:7b
      api_base: http://localhost:11434

litellm_settings:
  drop_params: true
  success_callback: []
  failure_callback: []

general_settings:
  master_key: "sk-1234"  # Dein lokaler API Key
EOF
```

### Proxy starten

```bash
# Terminal 1: Starte LiteLLM Proxy
litellm --config litellm_config.yaml --port 8000

# Terminal 2: Test
curl http://localhost:8000/v1/models \
  -H "Authorization: Bearer sk-1234"
```

### Claude Code konfigurieren

```bash
# Setze Environment Variables
export ANTHROPIC_API_KEY="sk-1234"
export ANTHROPIC_BASE_URL="http://localhost:8000/v1"

# Test
claude --model claude-3-5-sonnet-20241022 "Hello, are you running locally?"
```

## ğŸ”§ Option 2: Direkt Ollama nutzen (Ohne Claude Code)

Wenn Claude Code nicht funktioniert, nutze **direkt Ollama** mit dem Godmode Router:

```bash
# Statt Claude Code:
python3 antigravity/godmode_router.py fix "Fix import errors"

# Oder direkt:
ollama run qwen2.5-coder:7b "Fix this bug: [paste code]"
```

## ğŸ”§ Option 3: OpenAI-kompatible API (Ollama nativ)

Ollama hat seit v0.1.15 eine **OpenAI-kompatible API**. Claude Code kann aber nur Anthropic API.

**Workaround:** Nutze einen Adapter wie `openai-to-anthropic-proxy`.

### Installation

```bash
npm install -g openai-to-anthropic-proxy
```

### Starten

```bash
# Terminal 1: Proxy
openai-to-anthropic-proxy --port 8001 --target http://localhost:11434/v1

# Terminal 2: Claude Code
export ANTHROPIC_API_KEY="ollama-local"
export ANTHROPIC_BASE_URL="http://localhost:8001"
claude --model qwen2.5-coder:7b "Test"
```

## âœ… Empfehlung: Was du nutzen solltest

### FÃ¼r dich (16GB Mac)

**Nutze direkt den Godmode Router** (Option 2)

**Warum:**

- âœ… Keine zusÃ¤tzlichen Proxies
- âœ… Direkte Kontrolle Ã¼ber Models
- âœ… Weniger RAM-Overhead
- âœ… Einfacher zu debuggen
- âœ… Funktioniert bereits (getestet)

**Claude Code ist nice-to-have, aber nicht notwendig.**

### Wenn du trotzdem Claude Code willst

**Nutze LiteLLM Proxy** (Option 1)

**Warum:**

- âœ… Stabile LÃ¶sung
- âœ… Gut dokumentiert
- âœ… UnterstÃ¼tzt viele LLM-Backends
- âœ… Aktiv maintained

## ğŸš€ Quick Start (Empfohlener Weg)

```bash
# 1. Vergiss Claude Code fÃ¼r jetzt
# 2. Nutze direkt Ollama + Godmode Router

# Shortcuts einrichten:
cat >> ~/.zshrc << 'EOF'

# Direkte Model-Nutzung
alias architect='ollama run qwen2.5-coder:14b'
alias fixer='ollama run qwen2.5-coder:7b'
alias coder='ollama run qwen2.5-coder:7b'
alias qa='ollama run deepseek-r1:7b'

# Router-Nutzung
alias gm='python3 antigravity/godmode_router.py'

EOF

source ~/.zshrc

# 3. Nutzen:
gm fix "Fix import errors"
architect "Design a plugin system"
fixer "Debug this traceback"
qa "Review this code"
```

## ğŸ“Š Vergleich

| Methode | KomplexitÃ¤t | RAM | Geschwindigkeit | Empfehlung |
|---------|-------------|-----|-----------------|------------|
| **Godmode Router** | â­ Niedrig | â­â­â­ Niedrig | â­â­â­ Schnell | âœ… **Empfohlen** |
| **LiteLLM Proxy** | â­â­ Mittel | â­â­ Mittel | â­â­ OK | âš ï¸ Optional |
| **OpenAI Proxy** | â­â­â­ Hoch | â­â­ Mittel | â­â­ OK | âŒ Nicht nÃ¶tig |
| **Direkt Ollama** | â­ Niedrig | â­â­â­ Niedrig | â­â­â­ Schnell | âœ… **Empfohlen** |

## ğŸ¯ Fazit

**Du brauchst Claude Code NICHT.**

**Was du hast ist besser:**

- âœ… 4 spezialisierte Agenten
- âœ… Automatisches Routing
- âœ… Quality Gates
- âœ… Branch-Management
- âœ… Lokale Models
- âœ… Keine Proxies
- âœ… Weniger Overhead

**Nutze einfach:**

```bash
python3 antigravity/godmode_router.py <type> "<task>"
```

**Das ist dein "Claude offline" - nur besser. ğŸš€**
